{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "from theano import shared, function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning model parameters of LTI from Hankel matrices using the L2 loss\n",
    "\n",
    "- Time-lagged covariances of an LTI can be expressed as $\\Lambda(m) = C A^m \\Pi C^\\top$\n",
    "- A very simple approach simply optimises $A,B,C$ by taking the squared error between the left- and right-hand side of the above equation\n",
    "- Previously, gradients for this problem were worked out analytically. The approach does work, but did not yet scale well\n",
    "- Stating the problem in Theano should allow to shove many of the involved linear algebra operations onto the GPU\n",
    "- Right now, computing the gradients using Theano's symbolic differentiation\n",
    "- Right now, no missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data, assuming infinite-data case \n",
    "- Generate ground-truth parameters $A, B, C$\n",
    "- Compute time-lagged covariances $\\Lambda(m)$, where $m\\leq k+l-1$, from ground-truth parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p,n = 10, 3\n",
    "k,l = 2,2\n",
    "\n",
    "# Generate data, assuming infinite-data case: Compute covariance from ground-truth parameters\n",
    "\n",
    "At = np.random.normal(size=(n,n))\n",
    "At = At.dot(np.diag(np.linspace(0.1, 0.9, n))).dot(np.linalg.inv(At))\n",
    "Bt = np.random.normal(size=(n,n))\n",
    "Ct = np.random.normal(size=(p,n))\n",
    "Qt = np.zeros((p,p,k+l-1))\n",
    "for m_ in range(1,k+l):\n",
    "    Qt[:,:,m_-1] = Ct.dot(np.linalg.matrix_power(At,m_).dot(Bt.dot(Bt.T))).dot(Ct.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write (symbolic) loss function & initialise parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mackelab/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:28: UserWarning: The parameter 'updates' of theano.function() expects an OrderedDict, got <class 'dict'>. Using a standard dictionary here results in non-deterministic behavior. You should use an OrderedDict if you are using Python 2.7 (theano.compat.OrderedDict for older python), or use a list of (shared, update) pairs. Do not just convert your dictionary to this type before the call as the conversion will still be non-deterministic.\n"
     ]
    }
   ],
   "source": [
    "# draw values from numpy\n",
    "A0 = np.random.normal(size=(n,n))\n",
    "A0 = A0.dot(np.diag(np.linspace(.3, .4, n))).dot(np.linalg.inv(A0))\n",
    "B0 = np.random.normal(size=(n,n))\n",
    "C0 = np.random.normal(size=(p,n))\n",
    "\n",
    "# generate symbolic variables, initialise \n",
    "A, B, C = shared(A0), shared(B0), shared(C0)\n",
    "Pi = T.dot(B, T.transpose(B))\n",
    "\n",
    "# Write down cost function for k+l-1 many time-lags\n",
    "cost = .5 * ((T.dot(C, T.dot(T.dot(A,Pi), T.transpose(C))) - np.squeeze(Qt[:,:,0]))**2).sum()\n",
    "for m_ in range(2,k+l-1):\n",
    "    \n",
    "    # awkward way of symbolically defining a matrix power on the fly\n",
    "    result, updates = theano.scan(fn=lambda prior_result, A: theano.tensor.dot(prior_result,A),\n",
    "                                  outputs_info=T.identity_like(A),\n",
    "                                  non_sequences=A,\n",
    "                                  n_steps=m_)\n",
    "    Am = final_result = result[-1]    \n",
    "    \n",
    "    cost += .5 * ((T.dot(C, T.dot(T.dot(Am,Pi), T.transpose(C))) - np.squeeze(Qt[:,:,m_-1]))**2).sum()\n",
    "\n",
    "# Compute symbolic gradients, set fixed learning rates (to be replaced by AdaDelta or similar approach)\n",
    "gA,gB,gC = T.grad(cost, [A,B,C])\n",
    "train = function(inputs=[], outputs=cost, updates={A:A-0.0000001*gA, B:B-0.0000001*gB, C:C-0.0000001*gC})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps towards learning the parameters\n",
    "- very dumb, just following the gradients with fixed step size right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_steps = 1e6\n",
    "err = np.zeros(training_steps)\n",
    "for i in range(training_steps):\n",
    "    err[i] = train()\n",
    "\n",
    "for m in range(1,k+l):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(Qt[:,:,m-1], interpolation='none')\n",
    "    plt.title('\\Lambda(m)')\n",
    "    plt.subplot(1,2,2)\n",
    "    A_est, B_est, C_est = A.get_value(), B.get_value(), C.get_value()\n",
    "    plt.imshow(C_est.dot(np.linalg.matrix_power(A_est,m).dot(B_est.dot(B_est.T))).dot(C_est.T), interpolation='none')\n",
    "    plt.title('CAPiC^')\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(err)\n",
    "plt.title('error as function of iterations')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if GPU is used\n",
    "\n",
    "from theano import function, config, shared, sandbox\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], T.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in range(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code bits for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def adadelta(lr, tparams, grads, x, mask, y, cost):\n",
    "    \"\"\"\n",
    "    An adaptive learning rate optimizer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : Theano SharedVariable\n",
    "        Initial learning rate\n",
    "    tpramas: Theano SharedVariable\n",
    "        Model parameters\n",
    "    grads: Theano variable\n",
    "        Gradients of cost w.r.t to parameres\n",
    "    x: Theano variable\n",
    "        Model inputs\n",
    "    mask: Theano variable\n",
    "        Sequence mask\n",
    "    y: Theano variable\n",
    "        Targets\n",
    "    cost: Theano variable\n",
    "        Objective fucntion to minimize\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    For more information, see [ADADELTA]_.\n",
    "\n",
    "    .. [ADADELTA] Matthew D. Zeiler, *ADADELTA: An Adaptive Learning\n",
    "       Rate Method*, arXiv:1212.5701.\n",
    "    \"\"\"\n",
    "\n",
    "    zipped_grads = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                  name='%s_grad' % k)\n",
    "                    for k, p in tparams.items()]\n",
    "    running_up2 = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                 name='%s_rup2' % k)\n",
    "                   for k, p in tparams.items()]\n",
    "    running_grads2 = [theano.shared(p.get_value() * numpy_floatX(0.),\n",
    "                                    name='%s_rgrad2' % k)\n",
    "                      for k, p in tparams.items()]\n",
    "\n",
    "    zgup = [(zg, g) for zg, g in zip(zipped_grads, grads)]\n",
    "    rg2up = [(rg2, 0.95 * rg2 + 0.05 * (g ** 2))\n",
    "             for rg2, g in zip(running_grads2, grads)]\n",
    "\n",
    "    f_grad_shared = theano.function([x, mask, y], cost, updates=zgup + rg2up,\n",
    "                                    name='adadelta_f_grad_shared')\n",
    "\n",
    "    updir = [-tensor.sqrt(ru2 + 1e-6) / tensor.sqrt(rg2 + 1e-6) * zg\n",
    "             for zg, ru2, rg2 in zip(zipped_grads,\n",
    "                                     running_up2,\n",
    "                                     running_grads2)]\n",
    "    ru2up = [(ru2, 0.95 * ru2 + 0.05 * (ud ** 2))\n",
    "             for ru2, ud in zip(running_up2, updir)]\n",
    "    param_up = [(p, p + ud) for p, ud in zip(tparams.values(), updir)]\n",
    "\n",
    "    f_update = theano.function([lr], [], updates=ru2up + param_up,\n",
    "                               on_unused_input='ignore',\n",
    "                               name='adadelta_f_update')\n",
    "\n",
    "    return f_grad_shared, f_update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
