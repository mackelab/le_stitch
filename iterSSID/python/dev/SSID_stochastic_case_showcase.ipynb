{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hankel covariance matrix completion \n",
    "- $ H_{k,l} = (I_k \\otimes C) H^{xx}_{k,l} (I_l \\otimes C)^\\top $\n",
    "- $ H^{xx}_{k,l} = \\left[\\begin{array}{llll} A \\Pi & A^2 \\Pi & \\ldots & A^l \\Pi\\\\ A^2 \\Pi & A^3 \\Pi & \\ldots & A^{l+1} \\Pi\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ A^{k} \\Pi & A^{k+1} \\Pi & \\ldots & A^{k+l-1} \\Pi \\end{array} \\right] $\n",
    "- each block of the Hankel cov matrix $H_{k,l}$ exhibits the same structure of missing entries as does cov($y$.\n",
    "- We here first assume $A,\\Pi$ to be known, and apply Srini's idea of joint gradient descent on the whole $C$ to $H_{k,l}$. Then we learn $A, \\Pi$ with the other parameters fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import fmin_bfgs, check_grad\n",
    "import glob, os\n",
    "\n",
    "os.chdir('../core')\n",
    "import stitching_ssid as ssid\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p,n = 9,3\n",
    "k,l = n+1,n+1\n",
    "\n",
    "# create subpopulations\n",
    "sub_pops = (np.arange(0,p//2+1), np.arange(p//2,p))\n",
    "print('sub_pops', sub_pops)\n",
    "obs_idx, idx_grp, overlaps, overlap_grp, idx_overlap, Om, Ovw, Ovc = \\\n",
    "    ssid.get_subpop_stats(sub_pops, p, verbose=True)\n",
    "\n",
    "for rep in range(20):\n",
    "\n",
    "\n",
    "    C_true      = np.random.normal(size=(p,n))\n",
    "    A_true      = np.random.normal(size=(n,n)) #np.diag(np.linspace(0.5, 0.9, n)) #\n",
    "    \n",
    "    U,S,V = np.linalg.svd(A_true)\n",
    "    A_true = U.dot(np.diag(np.linspace(0.5, 0.9, n))).dot(V)\n",
    "\n",
    "    #B_true      = np.random.normal(size=(n,n))/np.sqrt(n)    \n",
    "    #Pi_true     = B_true.dot(B_true.T) #np.eye(n) \n",
    "    Pi_true = sp.stats.wishart(n, np.eye(n)).rvs()/n\n",
    "    B_true = np.linalg.cholesky(Pi_true)\n",
    "\n",
    "    Qs = ssid.comp_model_covariances({'A': A_true, 'Pi': Pi_true, 'C': C_true}, k+l, Om)\n",
    "    Qs_full = ssid.comp_model_covariances({'A': A_true, 'Pi': Pi_true, 'C': C_true}, k+l)\n",
    "\n",
    "\n",
    "    \n",
    "    A_0  = np.diag(np.random.uniform(low=0.7, high=0.8, size=n))\n",
    "    B_0  = np.random.normal(size=(n,n))\n",
    "    Pi_0 = B_0.dot(B_0.T)\n",
    "    C_0  = np.random.normal(size=(p,n))\n",
    "    pars_0 = np.hstack((A_0.reshape(n*n,),\n",
    "                        B_0.reshape(n*n,),\n",
    "                        C_0.reshape(p*n,)))\n",
    "    H_0 = ssid.yy_Hankel_cov_mat( C_0,A_0,Pi_0,k,l,~Om)\n",
    "    \n",
    "    f_i, g_i = ssid.l2_setup(k,l,n,Qs,Om,idx_grp,obs_idx)\n",
    "    \n",
    "    print('difference in gradient to finite-differencing value:', check_grad(f_i, g_i, pars_0))    \n",
    "    \n",
    "    pars_est = fmin_bfgs(f_i, pars_0, fprime=g_i, gtol=1e-20)    \n",
    "    A_est = pars_est[:n*n].reshape(n,n)\n",
    "    B_est = pars_est[n*n:2*n*n].reshape(n,n)\n",
    "    Pi_est = B_est.dot(B_est.T)\n",
    "    C_est = pars_est[-p*n:].reshape(p,n)\n",
    "    \n",
    "    pars_init = {'A': A_0, 'C': C_0, 'Pi': Pi_0, 'B': B_0}\n",
    "    pars_est  = {'A': A_est, 'C': C_est, 'Pi': Pi_est, 'B': B_est}\n",
    "    pars_true = {'A': A_true, 'C': C_true, 'Pi': Pi_true, 'B': B_true}\n",
    "    ssid.plot_outputs_l2_gradient_test(pars_true, pars_init, pars_est, k, l, Qs, \n",
    "                                       Qs_full, Om, Ovc, Ovw, f_i, g_i, if_flip = True)\n",
    "    \n",
    "    print('singular values of partial observability matrix for first subpop \\n')\n",
    "    _,s,_ = np.linalg.svd(ssid.observability_mat((A_true, C_true[sub_pops[0],:]), n))\n",
    "    print(s)\n",
    "\n",
    "    print('\\n singular values of partial observability matrix for second subpop \\n')\n",
    "    _,s,_ = np.linalg.svd(ssid.observability_mat((A_true, C_true[sub_pops[1],:]), n))\n",
    "    print(s)\n",
    "\n",
    "    print('\\n singular values of (noise) reachability matrix \\n')\n",
    "    _,s,_ = np.linalg.svd(ssid.observability_mat((A_true, B_true), n))\n",
    "    print(s)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large(r) systems tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p,n = 100,10\n",
    "k,l = n+1,n+1\n",
    "\n",
    "# create subpopulations\n",
    "sub_pops = (np.arange(0,90), np.arange(10,p))\n",
    "print('sub_pops', sub_pops)\n",
    "obs_idx, idx_grp, overlaps, overlap_grp, idx_overlap, Om, Ovw, Ovc = \\\n",
    "    ssid.get_subpop_stats(sub_pops, p, verbose=True)\n",
    "\n",
    "for rep in range(10):\n",
    "    \n",
    "    C_true      = np.random.normal(size=(p,n))\n",
    "    A_true      = np.diag(np.linspace(0.5, 0.9, n)) #\n",
    "\n",
    "    #B_true      = np.random.normal(size=(n,n))/np.sqrt(n)    \n",
    "    #Pi_true     = B_true.dot(B_true.T) #np.eye(n) \n",
    "    Pi_true = sp.stats.wishart(n, np.eye(n)).rvs()/n\n",
    "    B_true = np.linalg.cholesky(Pi_true)\n",
    "    \n",
    "    Qs = ssid.comp_model_covariances({'A': A_true, 'Pi': Pi_true, 'C': C_true}, k+l, Om)\n",
    "    Qs_full = ssid.comp_model_covariances({'A': A_true, 'Pi': Pi_true, 'C': C_true}, k+l)\n",
    "            \n",
    "    A_0  = np.diag(np.random.uniform(low=0.7, high=0.8, size=n))\n",
    "    B_0  = np.random.normal(size=(n,n))\n",
    "    Pi_0 = B_0.dot(B_0.T)\n",
    "    C_0  = np.random.normal(size=(p,n))\n",
    "    pars_0 = np.hstack((A_0.reshape(n*n,),\n",
    "                        B_0.reshape(n*n,),\n",
    "                        C_0.reshape(p*n,)))\n",
    "    H_0 = ssid.yy_Hankel_cov_mat( C_0,A_0,Pi_0,k,l,~Om)\n",
    "    \n",
    "    def f_i(pars):                        \n",
    "        A,B,C = pars[:n*n].reshape(n,n), pars[n*n:2*n*n].reshape(n,n), pars[-p*n:].reshape(p,n)\n",
    "        Pi = B.dot(B.T)\n",
    "        return ssid.f_l2_Hankel(C,A,Pi,k,l,Qs, Om)*np.sum(Om)*(k*l)\n",
    "    \n",
    "    def g_i(pars):        \n",
    "\n",
    "        A,B,C = pars[:n*n], pars[(n*n):(2*n*n)], pars[-p*n:]\n",
    "        return ssid.g_l2_Hankel(C,A,B,k,l,Qs,Om,idx_grp, obs_idx)\n",
    "    \n",
    "    print('difference in gradient to finite-differencing value:', check_grad(f_i, g_i, pars_0))    \n",
    "    \n",
    "    pars_est = fmin_bfgs(f_i, pars_0, fprime=g_i, gtol=1e-20)    \n",
    "    A_est = pars_est[:n*n].reshape(n,n)\n",
    "    B_est = pars_est[n*n:2*n*n].reshape(n,n)\n",
    "    Pi_est = B_est.dot(B_est.T)\n",
    "    C_est = pars_est[-p*n:].reshape(p,n)\n",
    "    \n",
    "    pars_init = {'A': A_0, 'C': C_0, 'Pi': Pi_0, 'B': B_0}\n",
    "    pars_est  = {'A': A_est, 'C': C_est, 'Pi': Pi_est, 'B': B_est}\n",
    "    pars_true = {'A': A_true, 'C': C_true, 'Pi': Pi_true, 'B': B_true}\n",
    "    ssid.plot_outputs_l2_gradient_test(pars_true, pars_init, pars_est, k, l, Qs, \n",
    "                                       Qs_full, Om, Ovc, Ovw, f_i, g_i, if_flip = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
