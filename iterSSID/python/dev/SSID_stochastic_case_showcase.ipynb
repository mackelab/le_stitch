{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hankel covariance matrix completion \n",
    "- $ H_{k,l} = (I_k \\otimes C) H^{xx}_{k,l} (I_l \\otimes C)^\\top $\n",
    "- $ H^{xx}_{k,l} = \\left[\\begin{array}{llll} A \\Pi & A^2 \\Pi & \\ldots & A^l \\Pi\\\\ A^2 \\Pi & A^3 \\Pi & \\ldots & A^{l+1} \\Pi\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ A^{k} \\Pi & A^{k+1} \\Pi & \\ldots & A^{k+l-1} \\Pi \\end{array} \\right] $\n",
    "- each block of the Hankel cov matrix $H_{k,l}$ exhibits the same structure of missing entries as does cov($y$.\n",
    "- We here first assume $A,\\Pi$ to be known, and apply Srini's idea of joint gradient descent on the whole $C$ to $H_{k,l}$. Then we learn $A, \\Pi$ with the other parameters fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import fmin_bfgs, check_grad\n",
    "import glob, os\n",
    "\n",
    "os.chdir('../core')\n",
    "import stitching_ssid as ssid\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "B = np.random.normal(size=(n,n))\n",
    "Pi = B.dot(B.T)\n",
    "A = np.random.normal(size=(n,n))\n",
    "\n",
    "print(A.dot(Pi).dot(A.T) + A.T.dot(Pi).dot(A))\n",
    "print((A+A.T).dot(Pi).dot((A+A.T).T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p,n = 21,3\n",
    "k,l = n+1,n+1\n",
    "\n",
    "# create subpopulations\n",
    "sub_pops = (np.arange(0,p//2+1), np.arange(p//2,p))\n",
    "print('sub_pops', sub_pops)\n",
    "obs_idx, idx_grp, overlaps, overlap_grp, idx_overlap, Om, Ovw, Ovc = \\\n",
    "    ssid.get_subpop_stats(sub_pops, p, verbose=True)\n",
    "\n",
    "for rep in range(10):\n",
    "\n",
    "\n",
    "    C_true      = np.random.normal(size=(p,n))\n",
    "    \n",
    "    V = np.random.normal(size=(n,n))\n",
    "    V /= np.sqrt(np.sum(V**2,axis=0)).reshape(1,-1)\n",
    "    A_true = V.dot(np.diag(np.linspace(0.7, 0.95, n))).dot(np.linalg.inv(V))\n",
    "    \n",
    "    #B_true      = np.random.normal(size=(n,n))/np.sqrt(n)    \n",
    "    #Pi_true     = B_true.dot(B_true.T) #np.eye(n) \n",
    "    Pi_true = sp.stats.wishart(n, np.eye(n)).rvs()/n\n",
    "    B_true = np.linalg.cholesky(Pi_true)\n",
    "\n",
    "    Qs = ssid.comp_model_covariances({'A': A_true, 'Pi': Pi_true, 'C': C_true}, k+l, Om)\n",
    "    Qs_full = ssid.comp_model_covariances({'A': A_true, 'Pi': Pi_true, 'C': C_true}, k+l)\n",
    "\n",
    "\n",
    "    \n",
    "    A_0  = np.diag(np.random.uniform(low=0.7, high=0.8, size=n))\n",
    "    B_0  = np.random.normal(size=(n,n))\n",
    "    Pi_0 = B_0.dot(B_0.T)\n",
    "    C_0  = np.random.normal(size=(p,n))\n",
    "    pars_0 = np.hstack((A_0.reshape(n*n,),\n",
    "                        B_0.reshape(n*n,),\n",
    "                        C_0.reshape(p*n,)))\n",
    "    H_0 = ssid.yy_Hankel_cov_mat( C_0,A_0,Pi_0,k,l,~Om)\n",
    "    \n",
    "    f_i, g_i = ssid.l2_setup(k,l,n,Qs,Om,idx_grp,obs_idx)\n",
    "    \n",
    "    print('difference in gradient to finite-differencing value:', check_grad(f_i, g_i, pars_0))    \n",
    "    \n",
    "    pars_est = fmin_bfgs(f_i, pars_0, fprime=g_i, gtol=1e-20)    \n",
    "    A_est = pars_est[:n*n].reshape(n,n)\n",
    "    B_est = pars_est[n*n:2*n*n].reshape(n,n)\n",
    "    Pi_est = B_est.dot(B_est.T)\n",
    "    C_est = pars_est[-p*n:].reshape(p,n)\n",
    "    \n",
    "    pars_init = {'A': A_0, 'C': C_0, 'Pi': Pi_0, 'B': B_0}\n",
    "    pars_est  = {'A': A_est, 'C': C_est, 'Pi': Pi_est, 'B': B_est}\n",
    "    pars_true = {'A': A_true, 'C': C_true, 'Pi': Pi_true, 'B': B_true}\n",
    "    ssid.plot_outputs_l2_gradient_test(pars_true, pars_init, pars_est, k, l, Qs, \n",
    "                                       Qs_full, Om, Ovc, Ovw, f_i, g_i, if_flip = True)\n",
    "    \n",
    "    print('singular values of partial observability matrix for first subpop \\n')\n",
    "    _,s,_ = np.linalg.svd(ssid.observability_mat((A_true, C_true[sub_pops[0],:]), n))\n",
    "    print(s)\n",
    "\n",
    "    print('\\n singular values of partial observability matrix for second subpop \\n')\n",
    "    _,s,_ = np.linalg.svd(ssid.observability_mat((A_true, C_true[sub_pops[1],:]), n))\n",
    "    print(s)\n",
    "\n",
    "    print('\\n singular values of (noise) reachability matrix \\n')\n",
    "    _,s,_ = np.linalg.svd(ssid.observability_mat((A_true, B_true), n))\n",
    "    print(s)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_rot(dim):\n",
    "    \"\"\"Return a random rotation matrix, drawn from the Haar distribution\n",
    "    (the only uniform distribution on SO(n)).\n",
    "    The algorithm is described in the paper\n",
    "    Stewart, G.W., 'The efficient generation of random orthogonal\n",
    "    matrices with an application to condition estimators', SIAM Journal\n",
    "    on Numerical Analysis, 17(3), pp. 403-409, 1980.\n",
    "    For more information see\n",
    "    http://en.wikipedia.org/wiki/Orthogonal_matrix#Randomization\"\"\"\n",
    "    H = np.eye(dim)\n",
    "    D = np.ones((dim,))\n",
    "    for n in range(1, dim):\n",
    "        x = np.random.normal(size=(dim-n+1,))\n",
    "        D[n-1] = np.sign(x[0])\n",
    "        x[0] -= D[n-1]*np.sqrt((x*x).sum())\n",
    "        # Householder transformation\n",
    "\n",
    "        Hx = np.eye(dim-n+1) - 2.*np.outer(x, x)/(x*x).sum()\n",
    "        mat = np.eye(dim)\n",
    "        mat[n-1:,n-1:] = Hx\n",
    "        H = np.dot(H, mat)\n",
    "    # Fix the last sign such that the determinant is 1\n",
    "    D[-1] = -D.prod()\n",
    "    H = (D*H.T).T\n",
    "    return H"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
